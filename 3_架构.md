

# 架构

存储时




分析时

图例

Hive的组件总体上可以分为以下几个部分：
用户接口（UI）、驱动、编译器、元数据（Hive系统参数数据）和执行引擎。


对外的接口UI包括以下几种：命令行CLI，Web界面、JDBC\/ODBC接口；
驱动：接收用户提交的查询HQL；
编译器：解析查询语句，执行语法分析，生成执行计划；
元数据Metadata：存放系统的表、分区、列、列类型等所有信息，以及对应的HDFS文件信息等；
执行引擎：执行执行计划，执行计划是一个有向无环图，执行引擎按照各个任务的依赖关系选择执行任务（Job）。


# metastore

虽然Hadoop天生支持平坦文件,但Hive可以使用目录结构来划分数据,以提高某些查询的性能.
为支持这些额外的功能,
Hive有一个全新且重要的组件,它就是用于存储schema信息的metastore.
这个metastore通常只有在关系型数据库中才有的.

，元数据库一般是通过关系型数据库MySQL或者支持JDBC驱动的关系型数据库来存储。元数据维护了库信息、表信息、列信息等所有内容


# 读时验证机制
  与传统数据库对表数据进行写时严重不同，Hive对数据的验证方式为读时模式，即只有在读表数据的时候，hive才检查解析具体的字段、shema等，从而保证了大数据量的快速加载。 既然hive采用的读时验证机制，那么 如果表schema与表文件内容不匹配，会发生什么呢？
  答案是hive会尽其所能的去读数据。如果schema中表有10个字段，而文件记录却只有3个字段，那么其中7个字段将为null；如果某些字段类型定位为数值类型，但是记录中却为非数值字符串，这些字段也将会被转换为null。简而言之，hive会努力catch读数据时遇到的错误，并努力返回。
  
      读时检查 hive
      写时检查 mysql
      
      读时检查 
          插入时不检查
          解耦
              HDFS 存储 提高存储效率
              Hive 分析  不符合就读取为null
              
      插入数据不匹配


